{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etemadir@ryerson.ca\n",
    "#2021-06-01 cikm2021\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from networkx import to_numpy_matrix\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "try:\n",
    "    import ujson as json\n",
    "except:\n",
    "    import json\n",
    "import math\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import ndcg_score \n",
    "import ml_metrics \n",
    "\n",
    "\n",
    "class CoExperts:    \n",
    "    def  __init__(self,data):        \n",
    "        self.dataset=data        \n",
    "    def init_model(self):\n",
    "        self.loadG()\n",
    "        self.d=32 #embeding dim\n",
    "        self.GCNW_1 =CoExperts.weight_variable((self.G.number_of_nodes(), 16)) \n",
    "        print(\"GCNW1=\",self.GCNW_1.shape)\n",
    "        self.GCNW_2 =CoExperts.weight_variable((self.GCNW_1.shape[1], self.d))\n",
    "        print(\"GCNW2=\",self.GCNW_2.shape)\n",
    "        \n",
    "        #MLP layer\n",
    "        self.regindim=6*self.GCNW_2.shape[1]+11\n",
    "        self.W1=CoExperts.weight_variable((self.regindim,16))\n",
    "        #self.W2=CoExperts.weight_variable((self.W1.shape[1],8))\n",
    "        #self.W3=CoExperts.weight_variable((self.W2.shape[1],16))\n",
    "        self.W4 = CoExperts.weight_variable2(self.W1.shape[1])\n",
    "        #self.W4 = CoExperts.weight_variable2(4*self.GCNW_2.shape[1])\n",
    "        self.b = tf.Variable(random.uniform(0, 1))\n",
    "        self.inputs=[]\n",
    "        self.outputs=[]      \n",
    "        \n",
    "        #kernel setting\n",
    "        self.n_bins=11\n",
    "        self.embedding_size=300        \n",
    "        self.lamb = 0.5\n",
    "        self.mus = CoExperts.kernal_mus(self.n_bins, use_exact=True)\n",
    "        self.sigmas = CoExperts.kernel_sigmas(self.n_bins, self.lamb, use_exact=True)\n",
    "        self.embeddings = tf.Variable(tf.random.uniform([self.vocab_size+1, self.embedding_size], -1.0, 1.0,dtype=tf.float32),dtype=tf.float32)\n",
    "       \n",
    "        \n",
    "        \n",
    "    def weight_variable(shape):\n",
    "        tmp = np.sqrt(6.0) / np.sqrt(shape[0]+shape[1])\n",
    "        initial = tf.random.uniform(shape, minval=-tmp, maxval=tmp)\n",
    "        return tf.Variable(initial,dtype=tf.float32)\n",
    "    \n",
    "    def weight_variable2(shape):\n",
    "        tmp = np.sqrt(6.0) / np.sqrt(shape)\n",
    "        initial = tf.random.uniform([shape,1], minval=-tmp, maxval=tmp)\n",
    "        return tf.Variable(initial,dtype=tf.float32)\n",
    "    \n",
    "    def load_traindata(self):\n",
    "        data_dir=self.dataset\n",
    "        INPUT=data_dir+\"/Data_LDA_Topics/train.txt\"        \n",
    "        fin=open(INPUT)\n",
    "        train=fin.readline().strip()\n",
    "        train_q_contributers={}\n",
    "        all_contrs=[]\n",
    "        while train:\n",
    "            data=train.split(\",\")\n",
    "            q_id=data[0]            \n",
    "            cntr_id=int(data[5]) \n",
    "            if q_id not in train_q_contributers:\n",
    "                train_q_contributers[q_id]=[cntr_id]\n",
    "            else:    \n",
    "                train_q_contributers[str(q_id)].append(cntr_id)\n",
    "            train=fin.readline().strip()\n",
    "            if cntr_id not in all_contrs:\n",
    "                all_contrs.append(cntr_id)\n",
    "        fin.close()\n",
    "        fin=open(data_dir+\"/properties_LDA_topics.txt\",\"r\",encoding=\"utf-8\")\n",
    "        RepoNum=int(fin.readline().split(\"=\")[1])\n",
    "        ExpertNum=int(fin.readline().split(\"=\")[1])\n",
    "        StartExpertID=RepoNum\n",
    "        fin.close()\n",
    "        \n",
    "        #load train data\n",
    "        self.train_data=[]\n",
    "        self.train_label=[]\n",
    "        \n",
    "        INPUT=data_dir+\"/Data_LDA_Topics/train.txt\"\n",
    "        fin_train=open(INPUT)        \n",
    "        train=fin_train.readline().strip()\n",
    "        \n",
    "        train_q_negs={}\n",
    "        while train:\n",
    "            data=train.split(\",\")            \n",
    "            lst=[int(data[0]),int(data[1])]\n",
    "            topics=[]\n",
    "            if len(data[2].strip())>0:\n",
    "                for d in data[2].split(\" \"):\n",
    "                    topics.append(int(d))\n",
    "            lst.append(topics)\n",
    "            languages=[]\n",
    "            if len(data[3].strip())>0:\n",
    "                for d in data[3].split(\" \"):\n",
    "                    languages.append(int(d))\n",
    "            lst.append(languages)\n",
    "            lst.append(int(data[4]))\n",
    "            lst1=lst.copy()\n",
    "            lst1.append(int(data[5]))\n",
    "            \n",
    "            self.train_data.append(lst1)\n",
    "            labels=data[6].split(\" \")\n",
    "            #print(labels)\n",
    "            if labels[3]=='0':\n",
    "                label=0\n",
    "            else:\n",
    "                label=(float(labels[0])/float(labels[3]))*15\n",
    "            self.train_label.append(label+4)\n",
    "            \n",
    "            #start add negative sample to train data\n",
    "            q_id=int(data[0])\n",
    "            flag=True\n",
    "            while flag:\n",
    "                ran=random.randint(0,ExpertNum-1)+StartExpertID\n",
    "                if (ran in all_contrs) and (ran not in train_q_contributers[str(q_id)]) and ( \n",
    "                    (str(q_id) not in train_q_negs) or  (ran not in train_q_negs[str(q_id)])) :\n",
    "                    lst1=lst.copy()\n",
    "                    lst1.append(int(ran))\n",
    "                    self.train_data.append(lst1)\n",
    "                    self.train_label.append(0.0)\n",
    "                    if str(q_id) not in train_q_negs:\n",
    "                        train_q_negs[str(q_id)]=[ran]\n",
    "                    else:\n",
    "                        train_q_negs[str(q_id)].append(ran)\n",
    "                    flag=False\n",
    "            #end add negative sample to train data\n",
    "            \n",
    "            train=fin_train.readline().strip()\n",
    "            \n",
    "        \n",
    "        fin_train.close()\n",
    "        \n",
    "        self.train_data=np.array(self.train_data)        \n",
    "        self.train_label=np.array(self.train_label)\n",
    "        \n",
    "        #load test as validation \n",
    "        INPUT=data_dir+\"/Data_LDA_Topics/test.txt\"        \n",
    "        fin=open(INPUT)\n",
    "        test=fin.readline().strip()\n",
    "        val_q_contributers={}\n",
    "        while test:\n",
    "            data=test.split(\",\")\n",
    "            q_id=data[0]            \n",
    "            cntr_id=int(data[5]) \n",
    "            if q_id not in val_q_contributers:\n",
    "                val_q_contributers[q_id]=[cntr_id]\n",
    "            else:    \n",
    "                val_q_contributers[str(q_id)].append(cntr_id)\n",
    "            test=fin.readline().strip()\n",
    "            \n",
    "        fin.close()\n",
    "        self.val_data=[]\n",
    "        self.val_label=[]\n",
    "        INPUT=data_dir+\"/Data_LDA_Topics/test.txt\"\n",
    "        fin_val=open(INPUT)        \n",
    "        val=fin_val.readline().strip()\n",
    "        val_q_negs={}\n",
    "        while val:\n",
    "            data=val.split(\",\")            \n",
    "            lst=[int(data[0]),int(data[1])]\n",
    "            topics=[]\n",
    "            if len(data[2].strip())>0:\n",
    "                for d in data[2].split(\" \"):\n",
    "                    topics.append(int(d))\n",
    "            lst.append(topics)\n",
    "            languages=[]\n",
    "            if len(data[3].strip())>0:\n",
    "                for d in data[3].split(\" \"):\n",
    "                    languages.append(int(d))\n",
    "            lst.append(languages)\n",
    "            lst.append(int(data[4]))\n",
    "            lst1=lst.copy()\n",
    "            lst1.append(int(data[5]))\n",
    "            \n",
    "            self.val_data.append(lst1)\n",
    "            labels=data[6].split(\" \")\n",
    "            #print(labels)\n",
    "            if labels[3]=='0':\n",
    "                label=0\n",
    "            else:\n",
    "                label=(float(labels[0])/float(labels[3]))*15\n",
    "            self.val_label.append(label+4)\n",
    "            \n",
    "            \n",
    "            #start add negative sample to train data\n",
    "            q_id=int(data[0])\n",
    "            flag=True\n",
    "            while flag:\n",
    "                ran=random.randint(0,ExpertNum-1)+StartExpertID\n",
    "                if (ran in all_contrs) and (ran not in val_q_contributers[str(q_id)]) and ( \n",
    "                    (str(q_id) not in train_q_negs) or  (ran not in train_q_negs[str(q_id)])):\n",
    "                    lst1=lst.copy()\n",
    "                    lst1.append(int(ran))\n",
    "                    self.val_data.append(lst1)\n",
    "                    self.val_label.append(0.0)\n",
    "                    if str(q_id) not in val_q_negs:\n",
    "                        val_q_negs[str(q_id)]=[ran]\n",
    "                    else:\n",
    "                        val_q_negs[str(q_id)].append(ran)\n",
    "                    flag=False\n",
    "            #end add negative sample to train data\n",
    "            val=fin_val.readline().strip()\n",
    "        fin_val.close()\n",
    "        \n",
    "        self.val_data=np.array(self.val_data)        \n",
    "        self.val_label=np.array(self.val_label)\n",
    "        \n",
    "        #load text for train\n",
    "        vocab=[]\n",
    "        \n",
    "        INPUT=data_dir+\"/vocabs.txt\"\n",
    "        fin=open( INPUT, \"r\", encoding=\"utf-8\")\n",
    "        line=fin.readline()\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            v = line.split(\" \")\n",
    "            if v[1].strip()!=\"\" and int(v[2].strip())>4:\n",
    "                vocab.append(v[1])\n",
    "            line=fin.readline().strip()\n",
    "        fin.close()    \n",
    "        self.vocab_size=len(vocab)\n",
    "        print(self.vocab_size)\n",
    "        \n",
    "        txt_cntrs={}\n",
    "        fin=open(data_dir+\"/Data_LDA_Topics/txt_contrs.txt\",\"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        txt_contrs_len=[]\n",
    "        while line:\n",
    "            data=line.split(\";;;\",1)\n",
    "            cid=data[0]\n",
    "            txt_cntrs[cid]={}\n",
    "            txts=data[1].split(\";;;\")\n",
    "            all_txt=\"\"\n",
    "            \n",
    "            for tx in txts:\n",
    "                tc=tx.split(\",\",1)\n",
    "                tccode=\"\"\n",
    "                for w in tc[1].split(\" \"):\n",
    "                    if w in vocab:\n",
    "                        tccode+=\" \"+str(vocab.index(w)+1)\n",
    "                all_txt+=\" \"+tccode.strip()\n",
    "                txt_cntrs[cid][tc[0]]=tccode.strip()\n",
    "            \n",
    "            txt_contrs_len.append(len(all_txt.split(\" \")))\n",
    "            line=fin.readline().strip()    \n",
    "        #print(txt_cntrs[\"466\"])\n",
    "        fin.close()\n",
    "        \n",
    "        txt_repos={}\n",
    "        fin=open(data_dir+\"/Data_LDA_Topics/txt_repo.txt\",\"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        txt_repos_len=[]\n",
    "        while line:\n",
    "            data=line.split(\",\",1)\n",
    "            rid=data[0]\n",
    "            txt_code=\"\"\n",
    "            for w in data[1].split(\" \"):\n",
    "                if w in vocab:\n",
    "                    txt_code+=\" \"+str(vocab.index(w)+1)\n",
    "            txt_code=txt_code.strip()\n",
    "            txt_repos_len.append(len(txt_code.split(\" \")))\n",
    "            if rid not in txt_repos:\n",
    "                txt_repos[rid]=txt_code\n",
    "            else:\n",
    "                txt_repos[rid].append(txt_code)\n",
    "            line=fin.readline().strip()    \n",
    "        #print(txt_repos[\"0\"])\n",
    "        fin.close() \n",
    "        #print(txt_repos)\n",
    "        self.max_q_len=math.floor(max(txt_repos_len))\n",
    "        self.max_d_len=math.floor(np.mean(txt_contrs_len))-self.max_q_len\n",
    "        print(self.max_q_len,self.max_d_len)\n",
    "        \n",
    "        self.qatext=[]\n",
    "        \n",
    "        for t in self.train_data:\n",
    "            repo_id=t[0]\n",
    "            repo_txt=[ int(wid) for wid in txt_repos[str(repo_id)].split(\" \")]\n",
    "            contrs_id=t[5]\n",
    "            \n",
    "            c_txt=\"\"\n",
    "            for rkey in txt_cntrs[str(contrs_id)].keys():\n",
    "                if rkey!=str(repo_id):\n",
    "                    c_txt+=\" \"+txt_cntrs[str(contrs_id)][rkey]\n",
    "            \n",
    "            cntrs_txt=[ int(wid) for wid in c_txt.strip().split(\" \")] #[:self.max_d_len]]\n",
    "            #cntrs_txt=[ int(wid) for wid in txt_cntrs[str(contrs_id)].split(\" \")[:self.max_d_len]]\n",
    "            self.qatext.append([repo_txt,cntrs_txt])   \n",
    "        self.qatext=np.array(self.qatext)\n",
    "        \n",
    "        self.val_data_text=[]\n",
    "        for t in self.val_data:\n",
    "            repo_id=t[0]\n",
    "            repo_txt=[ int(wid) for wid in txt_repos[str(repo_id)].split(\" \")]\n",
    "            contrs_id=t[5]\n",
    "            c_txt=\"\"\n",
    "            for rkey in txt_cntrs[str(contrs_id)].keys():\n",
    "                if rkey!=str(repo_id):\n",
    "                    c_txt+=\" \"+txt_cntrs[str(contrs_id)][rkey]\n",
    "            cntrs_txt=[ int(wid) for wid in c_txt.strip().split(\" \")]#[:self.max_d_len]]\n",
    "            #cntrs_txt=[ int(wid) for wid in txt_cntrs[str(contrs_id)].split(\" \")[:self.max_d_len]]\n",
    "            self.val_data_text.append([repo_txt,cntrs_txt])\n",
    "        \n",
    "        self.val_data_text=np.array(self.val_data_text)\n",
    "        #sys.exit(0)\n",
    "\n",
    "    def loadG(self):\n",
    "        INPUT=self.dataset+\"/Data_LDA_Topics/Gedgs.txt\"\n",
    "        self.G=nx.Graph();        \n",
    "        self.G=nx.read_weighted_edgelist(INPUT)\n",
    "        #self.G=nx.read_edgelist(INPUT)\n",
    "        order = [str(i) for i in range(len(self.G.nodes()))]\n",
    "        #print(order)\n",
    "        A_hat =to_numpy_matrix(self.G, nodelist=order,dtype=np.float32)\n",
    "        \n",
    "        I = np.eye(self.G.number_of_nodes(),dtype=np.float32)        \n",
    "        A_hat = A_hat + I        \n",
    "        print(\"A_hat=\",A_hat.shape)\n",
    "        \n",
    "        Dhat05 = np.array(np.sum(A_hat, axis=0),dtype=np.float32)[0]       \n",
    "        Dhat05 = np.matrix(np.diag(Dhat05),dtype=np.float32)        \n",
    "        Dhat05=fractional_matrix_power(Dhat05,-0.5)  \n",
    "        \n",
    "        self.DAD=np.array(Dhat05*A_hat*Dhat05,dtype=np.float32)\n",
    "        print(\"DAD=\",self.DAD.shape)\n",
    "    \n",
    "    #adopted from knrm paper ref:https://github.com/AdeDZY/K-NRM\n",
    "    @staticmethod\n",
    "    def kernal_mus(n_kernels, use_exact):\n",
    "        \"\"\"\n",
    "        get the mu for each guassian kernel. Mu is the middle of each bin\n",
    "        :param n_kernels: number of kernels (including exact match). first one is exact match\n",
    "        :return: l_mu, a list of mu.\n",
    "        \"\"\"\n",
    "        if use_exact:\n",
    "            l_mu = [1]\n",
    "        else:\n",
    "            l_mu = [2]\n",
    "        if n_kernels == 1:\n",
    "            return l_mu\n",
    "\n",
    "        bin_size = 2.0 / (n_kernels - 1)  # score range from [-1, 1]\n",
    "        l_mu.append(1 - bin_size / 2)  # mu: middle of the bin\n",
    "        for i in range(1, n_kernels - 1):\n",
    "            l_mu.append(l_mu[i] - bin_size)\n",
    "        return l_mu\n",
    "\n",
    "    #adpoted from knrm paper copied from knrm paper ref:https://github.com/AdeDZY/K-NRM\n",
    "    @staticmethod\n",
    "    def kernel_sigmas(n_kernels, lamb, use_exact):\n",
    "        \"\"\"\n",
    "        get sigmas for each guassian kernel.\n",
    "        :param n_kernels: number of kernels (including exactmath.)\n",
    "        :param lamb:\n",
    "        :param use_exact:\n",
    "        :return: l_sigma, a list of simga\n",
    "        \"\"\"\n",
    "        bin_size = 2.0 / (n_kernels - 1)\n",
    "        l_sigma = [0.00001]  # for exact match. small variance -> exact match\n",
    "        if n_kernels == 1:\n",
    "            return l_sigma\n",
    "\n",
    "        l_sigma += [bin_size * lamb] * (n_kernels - 1)\n",
    "        return l_sigma\n",
    "    \n",
    "    def q_a_rbf(self,inputs_q,inputs_d):    \n",
    "        # look up embeddings for each term. [nbatch, qlen, emb_dim]\n",
    "\n",
    "        self.max_q_len=len(inputs_q[0])\n",
    "        self.max_d_len=len(inputs_d[0])\n",
    "        \n",
    "        q_embed = tf.nn.embedding_lookup(self.embeddings, inputs_q, name='qemb')\n",
    "        d_embed = tf.nn.embedding_lookup(self.embeddings, inputs_d, name='demb')\n",
    "        batch_size=1\n",
    "        \n",
    "        # normalize and compute similarity matrix using l2 norm         \n",
    "        norm_q = tf.sqrt(tf.reduce_sum(tf.square(q_embed), 2))\n",
    "        #print(norm_q)\n",
    "        norm_q=tf.reshape(norm_q,(len(norm_q),len(norm_q[0]),1))\n",
    "        #print(norm_q)\n",
    "        normalized_q_embed = q_embed / norm_q\n",
    "        #print(normalized_q_embed)\n",
    "        norm_d = tf.sqrt(tf.reduce_sum(tf.square(d_embed), 2))\n",
    "        norm_d=tf.reshape(norm_d,(len(norm_d),len(norm_d[0]),1))\n",
    "        normalized_d_embed = d_embed / norm_d\n",
    "        #print(normalized_d_embed)\n",
    "        tmp = tf.transpose(normalized_d_embed, perm=[0, 2, 1])\n",
    "        #print(tmp)\n",
    "        sim =tf.matmul(normalized_q_embed, tmp)\n",
    "        #print(sim)        \n",
    "        # compute gaussian kernel\n",
    "        rs_sim = tf.reshape(sim, [batch_size, self.max_q_len, self.max_d_len, 1])\n",
    "        #print(rs_sim)\n",
    "        \n",
    "        tmp = tf.exp(-tf.square(tf.subtract(rs_sim, self.mus)) / (tf.multiply(tf.square(self.sigmas), 2)))\n",
    "        #print(tmp)\n",
    "        \n",
    "        feats = []  # store the soft-TF features from each field.\n",
    "        # sum up gaussian scores\n",
    "        kde = tf.reduce_sum(tmp, [2])\n",
    "        kde = tf.math.log(tf.maximum(kde, 1e-10)) * 0.01  # 0.01 used to scale down the data.\n",
    "        # [batch_size, qlen, n_bins]\n",
    "        \n",
    "        #print(kde)\n",
    "        # aggregated query terms\n",
    "        # q_weights = [1, 1, 0, 0...]. Works as a query word mask.\n",
    "        # Support query-term weigting if set to continous values (e.g. IDF).\n",
    "        \n",
    "        #q_weights=np.where(np.array(inputs_q)>0,1,0)\n",
    "        #q_weights=tf.dtypes.cast(q_weights, tf.float32)\n",
    "        #q_weights = tf.reshape(q_weights, shape=[batch_size, self.max_q_len, 1])\n",
    "        \n",
    "        aggregated_kde = tf.reduce_sum(kde , [1])  # [batch, n_bins]   *q_weights\n",
    "        #print( aggregated_kde)\n",
    "        feats.append(aggregated_kde) # [[batch, nbins]]\n",
    "        feats_tmp = tf.concat( feats,1)  # [batch, n_bins]\n",
    "        #print (\"batch feature shape:\", feats_tmp.get_shape())\n",
    "        \n",
    "        # Reshape. (maybe not necessary...)\n",
    "        feats_flat = tf.reshape(feats_tmp, [-1, self.n_bins])\n",
    "        feats_flat2=tf.reshape(feats_flat, [1,self.n_bins])\n",
    "        \n",
    "        return(feats_flat2)               \n",
    "        \n",
    "    def GCN_layer1(self):      \n",
    "        return tf.matmul(self.DAD,self.GCNW_1)\n",
    "    \n",
    "    def GCN_layer2(self,i,X):\n",
    "        a=tf.matmul([self.DAD[i,:]],X) \n",
    "        return tf.matmul(a,self.GCNW_2)\n",
    "    \n",
    "    def GCN_layers(self,i):\n",
    "        H_1 = self.GCN_layer1()        \n",
    "        H_2 = self.GCN_layer2(i,H_1)\n",
    "        return H_2\n",
    "              \n",
    "    def model_test(self):\n",
    "        \"\"\"used for test\"\"\"\n",
    "        embed=[]\n",
    "        #print(self.inputs)\n",
    "        for k in range(len(self.inputs)): \n",
    "            ind=self.inputs[k]\n",
    "            qtext=[self.qatextinput[k][0]]\n",
    "            #print(qtext)\n",
    "            atext=[self.qatextinput[k][1]]\n",
    "            q_a_rbf=self.q_a_rbf(qtext,atext)\n",
    "            \n",
    "            repoembed=tf.constant([np.zeros(self.d)],dtype=tf.float32)\n",
    "                        \n",
    "            #print(qembed)\n",
    "            ownerembed=self.GCN_layers(ind[1])\n",
    "            LDA_topic_embed=self.GCN_layers(ind[4])\n",
    "            \n",
    "            contributer_embed=self.GCN_layers(ind[5])\n",
    "            \n",
    "            if len(ind[2])>0:\n",
    "                i=1             \n",
    "                lst=[self.GCN_layers(ind[2][0])]\n",
    "                for indx in  ind[2][1:]:\n",
    "                    lst.append(self.GCN_layers(indx))\n",
    "                    i=i+1\n",
    "                topicsembed=tf.math.reduce_sum(lst, axis=0)/i \n",
    "            else:  \n",
    "                topicsembed=tf.constant([np.zeros(self.d)],dtype=tf.float32)\n",
    "            \n",
    "            if len(ind[3])>0:\n",
    "                i=1             \n",
    "                lst=[self.GCN_layers(ind[3][0])]\n",
    "                for indx in  ind[3][1:]:\n",
    "                    lst.append(self.GCN_layers(indx))\n",
    "                    i=i+1\n",
    "                languagesembed=tf.math.reduce_sum(lst, axis=0)/i \n",
    "            else:  \n",
    "                languagesembed=tf.constant([np.zeros(self.d)],dtype=tf.float32)\n",
    "            \n",
    "            embed1=tf.concat([q_a_rbf,repoembed,ownerembed,topicsembed,languagesembed,LDA_topic_embed,contributer_embed],1, name='concat')\n",
    "            #embed1=tf.concat([qembed,askerembed,answererembed,tagsembed],1, name='concat')\n",
    "            embed.append(embed1)\n",
    "        embed=tf.reshape(embed,[len(self.inputs),self.regindim])    \n",
    "        #return  tf.reshape(tf.matmul(embed,self.W4),[len(self.inputs)]) + self.b\n",
    "        #print(embed)\n",
    "        #print(len(embed))\n",
    "        #print(len(embed[0]))\n",
    "        w1out=tf.nn.tanh(tf.matmul(embed,self.W1))\n",
    "        #print(w1out.shape)\n",
    "        #w2out=tf.nn.tanh(tf.matmul(w1out,self.W2))\n",
    "        #print(w2out.shape)\n",
    "        #w3out=tf.nn.tanh(tf.matmul(w2out,self.W3))\n",
    "        #print(w3out.shape)   \n",
    "        return  tf.reshape(tf.matmul(w1out,self.W4),[len(self.inputs)]) + self.b\n",
    "    \n",
    "    \n",
    "    def model(self):\n",
    "        embed=[]\n",
    "        #print(self.inputs)\n",
    "        for k in range(len(self.inputs)): \n",
    "            ind=self.inputs[k]\n",
    "            qtext=[self.qatextinput[k][0]]\n",
    "            #print(qtext)\n",
    "            atext=[self.qatextinput[k][1]]\n",
    "            q_a_rbf=self.q_a_rbf(qtext,atext)\n",
    "            \n",
    "            repoembed=self.GCN_layers(ind[0])\n",
    "            #print(qembed)\n",
    "            ownerembed=self.GCN_layers(ind[1])\n",
    "            LDA_topic_embed=self.GCN_layers(ind[4])\n",
    "            contributer_embed=self.GCN_layers(ind[5])\n",
    "            \n",
    "            if len(ind[2])>0:\n",
    "                i=1             \n",
    "                lst=[self.GCN_layers(ind[2][0])]\n",
    "                for indx in  ind[2][1:]:\n",
    "                    lst.append(self.GCN_layers(indx))\n",
    "                    i=i+1\n",
    "                topicsembed=tf.math.reduce_sum(lst, axis=0)/i \n",
    "            else:  \n",
    "                topicsembed=tf.constant([np.zeros(self.d)],dtype=tf.float32)\n",
    "            \n",
    "            if len(ind[3])>0:\n",
    "                i=1             \n",
    "                lst=[self.GCN_layers(ind[3][0])]\n",
    "                for indx in  ind[3][1:]:\n",
    "                    lst.append(self.GCN_layers(indx))\n",
    "                    i=i+1\n",
    "                languagesembed=tf.math.reduce_sum(lst, axis=0)/i \n",
    "            else:  \n",
    "                languagesembed=tf.constant([np.zeros(self.d)],dtype=tf.float32)\n",
    "            \n",
    "            embed1=tf.concat([q_a_rbf,repoembed,ownerembed,topicsembed,languagesembed,LDA_topic_embed,contributer_embed],1, name='concat')\n",
    "            #embed1=tf.concat([qembed,askerembed,answererembed,tagsembed],1, name='concat')\n",
    "            embed.append(embed1)\n",
    "        embed=tf.reshape(embed,[len(self.inputs),self.regindim])    \n",
    "        #return  tf.reshape(tf.matmul(embed,self.W4),[len(self.inputs)]) + self.b\n",
    "        #print(embed)\n",
    "        #print(len(embed))\n",
    "        #print(len(embed[0]))\n",
    "        w1out=tf.nn.tanh(tf.matmul(embed,self.W1))\n",
    "        #print(w1out.shape)\n",
    "        #w2out=tf.nn.tanh(tf.matmul(w1out,self.W2))\n",
    "        #print(w2out.shape)\n",
    "        #w3out=tf.nn.tanh(tf.matmul(w2out,self.W3))\n",
    "        #print(w3out.shape)   \n",
    "        return  tf.reshape(tf.matmul(w1out,self.W4),[len(self.inputs)]) + self.b\n",
    "        \n",
    "    \n",
    "    def loss(self):\n",
    "        self.L= tf.reduce_mean(tf.square(self.model() - self.outputs))\n",
    "        return self.L  \n",
    "        \n",
    "    def train(self,model_save_name,l_rate=0.0005,eps=5e-7,decay_step=500,epoch_nums=20): \n",
    "        self.load_traindata()\n",
    "        self.init_model()\n",
    "        \n",
    "        print(\"train data loaded!!\")\n",
    "      \n",
    "        val_len=math.ceil(0.1*len(self.train_data))\n",
    "       \n",
    "        len_train_data=len(self.train_data)\n",
    "        \n",
    "        loss_=0\n",
    "        epochs = range(epoch_nums)\n",
    "        self.batch_size=1\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        self.l_rate=l_rate\n",
    "        self.epsilon=eps\n",
    "        self.decay_step=decay_step\n",
    "        self.optimizer=\"Adam\"\n",
    "        decayed_lr = tf.compat.v1.train.exponential_decay(self.l_rate,\n",
    "                                        global_step,self.decay_step,\n",
    "                                        0.95, staircase=True)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=decayed_lr,epsilon=self.epsilon)#(decayed_lr,epsilon=5e-6)\n",
    "        logfile=open(self.dataset+\"/log.txt\",\"w\")\n",
    "        t_loss=[]\n",
    "        v_loss=[]\n",
    "        eps=[]\n",
    "        \n",
    "        for epoch in epochs:\n",
    "            ind_new=[i for i in range(len_train_data)]\n",
    "            np.random.shuffle(ind_new)\n",
    "            self.train_data=self.train_data[ind_new,]\n",
    "            self.train_label=self.train_label[ind_new,]           \n",
    "            self.qatext=self.qatext[ind_new,] \n",
    "            \n",
    "            start=0\n",
    "            end=0\n",
    "            for i in range(math.ceil(len_train_data/self.batch_size)):\n",
    "                if ((i+1)*self.batch_size)<len_train_data:                    \n",
    "                    start=i*self.batch_size\n",
    "                    end=(i+1)*self.batch_size\n",
    "                else:                    \n",
    "                    start=i*self.batch_size\n",
    "                    end=len_train_data\n",
    "                    \n",
    "                self.inputs=self.train_data[start:end]\n",
    "                self.outputs=self.train_label[start:end]\n",
    "                self.qatextinput=self.qatext[start:end]\n",
    "                opt.minimize(self.loss, var_list=[self.embeddings,self.GCNW_1,self.GCNW_2,self.W1,self.W4,self.b])\n",
    "                   \n",
    "                loss_+=self.L \n",
    "                \n",
    "                global_step.assign_add(1)\n",
    "                opt._decayed_lr(tf.float32)\n",
    "                \n",
    "                #print(self.Loss)\n",
    "                #sys.exit(0)\n",
    "                if (i+1)%50==0:                    \n",
    "                    rep=(epoch*math.ceil(len_train_data/self.batch_size))+((i+1))\n",
    "                    txt='Epoch %2d: i  %2d  out of  %4d     loss=%2.5f' %(epoch, i*self.batch_size, len_train_data, loss_/(rep))\n",
    "                    logfile.write(txt+\"\\n\")\n",
    "                    print(txt)    \n",
    "            opt._decayed_lr(tf.float32)\n",
    "            #print(self.W4)\n",
    "            #validate the results\n",
    "            \n",
    "            print(\"\\n************\\nValidation started....\\n\")\n",
    "            val_loss=0\n",
    "            \n",
    "            for ii in range(math.ceil(val_len/self.batch_size)):\n",
    "                if ((ii+1)*self.batch_size)<val_len:\n",
    "                    start=ii*self.batch_size\n",
    "                    end=(ii+1)*self.batch_size\n",
    "                else:\n",
    "                    start=ii*self.batch_size\n",
    "                    end=val_len\n",
    "                self.inputs=self.val_data[start:end]\n",
    "                self.outputs=self.val_label[start:end]\n",
    "                self.qatextinput=self.val_data_text[start:end]\n",
    "                \n",
    "                val_loss+=self.loss()\n",
    "                #print(self.loss())\n",
    "                #print(val_loss)\n",
    "                if (ii+1)%50==0:                   \n",
    "                    txt='Epoch %2d: ii  %2d  out of  %4d     validation loss=%2.5f' %(epoch, ii*self.batch_size, val_len, val_loss/(ii+1))\n",
    "                    logfile.write(txt+\"\\n\")\n",
    "                    print(txt)\n",
    "            txt='Epoch %2d: ii  %2d  out of  %4d     validation loss=%2.5f' %(epoch, ii*self.batch_size, val_len, val_loss/(ii+1))\n",
    "            logfile.write(txt+\"\\n\")\n",
    "            print(txt)\n",
    "            \n",
    "            if epoch%1==0:\n",
    "                pkl_filename =self.dataset+ \"/Data_LDA_Topics/st_co/pickle_model.pkl_\"+model_save_name+str(epoch)\n",
    "                with open(pkl_filename, 'wb') as file:\n",
    "                    pickle.dump(self, file)\n",
    "                print(\"model was saved\")\n",
    "            t_loss.append(loss_/(rep))\n",
    "            v_loss.append(val_loss/math.ceil(val_len/self.batch_size))\n",
    "            eps.append(epoch)\n",
    "            plt.figure(figsize=(10,7))\n",
    "            plt.plot(eps,t_loss,'r-o',label = \"train loss\")\n",
    "            plt.plot(eps,v_loss,'b-*',label = \"validation loss\")\n",
    "            plt.title(\"train and validation losses\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            #plt.savefig(self.dataset+ \"/QRouting_noTags_noText/QRresults1/loss.png\")\n",
    "            plt.show()\n",
    "        print(\"train model done!!\")\n",
    "        logfile.close() \n",
    "\n",
    "    def test_all(data_name,path, modelname,model_num):    \n",
    "        \"\"\"ranks all experts\"\"\"\n",
    "        pkl_filename =data_name+path+\"/st_co/\"+modelname+model_num\n",
    "        # Load from file\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            ob = pickle.load(file)\n",
    "        print(\"model was loaded!!\")\n",
    "        \n",
    "        cntrs_ids=[]\n",
    "        \n",
    "        file_result=data_name+path+\"/st_co/\"+modelname+\"_all_result_allexperts.txt\"\n",
    "        if not os.path.isfile(file_result):\n",
    "            f_result=open(file_result,\"w\")\n",
    "        else:\n",
    "            f_result=open(file_result,\"a+\")\n",
    "            \n",
    "        fin=open(data_name+path+\"txt_contrs.txt\",\"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            data=line.split(\";;;\",1)\n",
    "            cid=data[0]            \n",
    "            cntrs_ids.append(int(cid))\n",
    "            line=fin.readline().strip()    \n",
    "        #print(txt_cntrs[\"466\"])\n",
    "        fin.close()\n",
    "         \n",
    "        INPUT=data_name+path+\"test.txt\"        \n",
    "        fin_test=open(INPUT)\n",
    "        test=fin_test.readline().strip()\n",
    "        test_data=[]\n",
    "        tst_q_ids=[]\n",
    "        tst_q_contributers={}\n",
    "        \n",
    "        while test:\n",
    "            data=test.split(\",\")\n",
    "            \n",
    "            q_id=int(data[0])            \n",
    "            cntr_id=int(data[5])\n",
    "            \n",
    "            if q_id not in tst_q_ids:\n",
    "                q_owner_id=int(data[1])\n",
    "            \n",
    "                topic_ids=[]            \n",
    "                if data[2].strip()!='':\n",
    "                    for tid in data[2].split(\" \"):\n",
    "                        topic_ids.append(int(tid))\n",
    "                l_ids=[]            \n",
    "                if data[3].strip()!='':\n",
    "                    for lid in data[3].split(\" \"):\n",
    "                        l_ids.append(int(lid))\n",
    "\n",
    "                lda_topic_id=int(data[4])\n",
    "                \n",
    "                tst_q_ids.append(q_id)\n",
    "                tst_q_contributers[str(q_id)]=[cntr_id]\n",
    "                test_data.append([q_id,q_owner_id,topic_ids,l_ids,lda_topic_id])\n",
    "                \n",
    "            else:\n",
    "                tst_q_contributers[str(q_id)].append(cntr_id)\n",
    "            test=fin_test.readline().strip()\n",
    "        fin_test.close() \n",
    "        \n",
    "        data_dir=data_name+\"/\"\n",
    "        fin=open(data_dir +\"properties_LDA_topics.txt\",\"r\",encoding=\"utf-8\")\n",
    "        RepoNum=int(fin.readline().split(\"=\")[1])\n",
    "        ExpertNum=int(fin.readline().split(\"=\")[1])\n",
    "        StartExpertID=RepoNum\n",
    "        fin.close()\n",
    "        \n",
    "        total_match=0\n",
    "        k=1\n",
    "        filename =data_name+path+\"/st_co/\"+modelname+model_num+\"_results1_allexperts.txt\"\n",
    "        \n",
    "        fout=open(filename,\"w\")\n",
    "        fout_test=open(data_name+path+\"/st_co/\"+modelname+model_num+\"_test_results1_allexperts.txt\",\"w\")\n",
    "        \n",
    "        #load text for train\n",
    "        vocab=[]\n",
    "        \n",
    "        INPUT=data_name+\"/vocabs.txt\"\n",
    "        fin=open( INPUT, \"r\", encoding=\"utf-8\")\n",
    "        line=fin.readline()\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            v = line.split(\" \")\n",
    "            if v[1].strip()!=\"\" and int(v[2].strip())>4:\n",
    "                vocab.append(v[1])\n",
    "            line=fin.readline().strip()\n",
    "        fin.close()    \n",
    "        \n",
    "        \n",
    "        txt_cntrs={}\n",
    "        fin=open(data_name+path+\"txt_contrs.txt\",\"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        txt_contrs_len=[]\n",
    "        while line:\n",
    "            data=line.split(\";;;\",1)\n",
    "            cid=data[0]\n",
    "            txt_cntrs[cid]={}\n",
    "            txts=data[1].split(\";;;\")\n",
    "            all_txt=\"\"\n",
    "            \n",
    "            for tx in txts:\n",
    "                tc=tx.split(\",\",1)\n",
    "                tccode=\"\"\n",
    "                for w in tc[1].split(\" \"):\n",
    "                    if w in vocab:\n",
    "                        tccode+=\" \"+str(vocab.index(w)+1)\n",
    "                all_txt+=\" \"+tccode.strip()\n",
    "                txt_cntrs[cid][tc[0].strip()]=tccode.strip()\n",
    "            \n",
    "            txt_contrs_len.append(len(all_txt.split(\" \")))\n",
    "            line=fin.readline().strip()    \n",
    "        #print(txt_cntrs[\"466\"])\n",
    "        fin.close()\n",
    "        \n",
    "        txt_repos={}\n",
    "        fin=open(data_name+path+\"txt_repo.txt\",\"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        txt_repos_len=[]\n",
    "        while line:\n",
    "            data=line.split(\",\",1)\n",
    "            rid=data[0]\n",
    "            txt_code=\"\"\n",
    "            for w in data[1].split(\" \"):\n",
    "                if w in vocab:\n",
    "                    txt_code+=\" \"+str(vocab.index(w)+1)\n",
    "            txt_code=txt_code.strip()\n",
    "            txt_repos_len.append(len(txt_code.split(\" \")))\n",
    "            if rid not in txt_repos:\n",
    "                txt_repos[rid]=txt_code\n",
    "            else:\n",
    "                txt_repos[rid].append(txt_code)\n",
    "            line=fin.readline().strip()    \n",
    "        #print(txt_repos[\"0\"])\n",
    "        fin.close() \n",
    "        #print(txt_repos)\n",
    "        max_q_len=ob.max_q_len\n",
    "        max_d_len=ob.max_d_len\n",
    "        print(max_q_len,max_d_len)\n",
    "        \n",
    "        print(\"num test qs=\",len(test_data))\n",
    "        for tst_q in test_data:\n",
    "            #print(\"\\n\\ntest q \"+str(k)+\":\")\n",
    "            inputs=[]\n",
    "            true_cntrs=tst_q_contributers[str(tst_q[0])]\n",
    "            repo_id=int(tst_q[0])\n",
    "            \n",
    "            inputtext=[]\n",
    "            \n",
    "            repo_txt=[ int(wid) for wid in txt_repos[str(repo_id)].split(\" \")]            \n",
    "            \n",
    "            \n",
    "            for c_id in cntrs_ids:\n",
    "                input_item=tst_q.copy()\n",
    "                input_item.append(c_id)\n",
    "                inputs.append(input_item)\n",
    "                c_txt=\"\"\n",
    "                for rkey in txt_cntrs[str(c_id)].keys():\n",
    "                    if rkey!=str(repo_id):\n",
    "                        c_txt+=\" \"+txt_cntrs[str(c_id)][rkey]\n",
    "                cntrs_txt=[ int(wid) for wid in c_txt.strip().split(\" \")]#[:max_d_len]]\n",
    "                inputtext.append([repo_txt,cntrs_txt])\n",
    "            print(tst_q[0])\n",
    "\n",
    "            #print(len(inputs))\n",
    "            ob.inputs=inputs\n",
    "            ob.qatextinput=inputtext\n",
    "            scores=ob.model_test().numpy()\n",
    "            ids=cntrs_ids.copy()\n",
    "            \n",
    "            sorted_scores,sorted_ids=(list(t) for t in zip(*sorted(zip(scores, ids),reverse=True)) ) \n",
    "            top_cntrs=sorted_ids[:len(true_cntrs)]\n",
    "            match=len( set(top_cntrs).intersection(set(true_cntrs))) /len(true_cntrs)\n",
    "            #print(\"match\\%:\"+str(match))\n",
    "            total_match+=match\n",
    "            #print(\"ave match\\%:\"+str(total_match/k))\n",
    "            fout.write(\"match\\%:\"+str(match)+\"ave match\\%:\"+str(total_match/k)+\"\\n\")\n",
    "            fout.flush()\n",
    "            k+=1\n",
    "            #write rsults into file\n",
    "            true_contrs=' '.join([str(pcid) for pcid in true_cntrs])\n",
    "            all_contrs=' '.join([str(pcid) for pcid in cntrs_ids])\n",
    "            res=\"\"\n",
    "            for ind in range(len(sorted_ids)):\n",
    "                res+=\" \"+str(sorted_ids[ind])+ \" \"+ str(sorted_scores[ind])\n",
    "            fout_test.write(str(tst_q[0])+\",\"+true_contrs.strip()+\",\"+all_contrs.strip()+\",\"+res.strip()+\"\\n\")\n",
    "            fout_test.flush()\n",
    "            fout_test.flush()    \n",
    "            \n",
    "            \n",
    "        print(\"ave match\\%:\"+str(total_match/k))    \n",
    "        print(\"test_model done!!\") \n",
    "        f_result.write(\"\\nmodel=\"+modelname+model_num+\"\\n\")\n",
    "        f_result.write(\"ave match\\%:\"+str(total_match/k))\n",
    "        f_result.close()\n",
    "        \n",
    "        fout.close()\n",
    "        fout_test.close()\n",
    "         \n",
    "    def discover_teams_baselineMatchZoo_all_cikm2021(self,path,alg,model,team_size):        \n",
    "        #load baseline results\n",
    "        #load baseline results\n",
    "        test_statistics={}\n",
    "        fin=open(path+\"test.txt\",\"r\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            data=line.split(\",\")\n",
    "            repo_id=data[0].strip()\n",
    "            numadds=int(data[6].split(\" \")[0])\n",
    "            if repo_id not in test_statistics:\n",
    "                test_statistics[repo_id]={\"true\":[int(data[5])],\"adds\":[numadds],\"neg\":[]}\n",
    "            else:\n",
    "                test_statistics[repo_id][\"true\"].append(int(data[5]))\n",
    "                test_statistics[repo_id][\"adds\"].append(numadds)\n",
    "            line=fin.readline().strip()\n",
    "        fin.close()\n",
    "        \n",
    "        \n",
    "        cntrs_ids=[]\n",
    "        fin=open(path+\"txt_contrs.txt\",\"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            data=line.split(\";;;\",1)\n",
    "            cid=int(data[0])\n",
    "            cntrs_ids.append(cid)\n",
    "            line=fin.readline().strip()    \n",
    "        #print(txt_cntrs[\"466\"])\n",
    "        fin.close()\n",
    "        fin=open(path +\"G_properties.txt\",\"r\",encoding=\"utf-8\")\n",
    "        self.N=int(fin.readline().split(\"=\")[1])\n",
    "        self.NumRepos=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        self.NumExperts=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        fin.close()\n",
    "        #load baseline results\n",
    "        baseline_results=np.loadtxt(path+alg+\"/\"+model)\n",
    "        \n",
    "        #shortest path\n",
    "        G_avr_sh_p=0\n",
    "        num_p=0\n",
    "        sh_path={}\n",
    "        fin=open(path+\"G_shortest_path.txt\",\"r\")\n",
    "        line=fin.readline()\n",
    "        while line:\n",
    "            data=line.split(\" \")\n",
    "            i=data[0]+\",\"+data[1]\n",
    "            sh=int(data[3])\n",
    "            #print(i,sh)\n",
    "            sh_path[i]=sh\n",
    "            G_avr_sh_p+=sh\n",
    "            num_p+=1\n",
    "            #sys.exit(0)\n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        print(\"G avr sh path=\"+str(G_avr_sh_p/num_p))\n",
    "        start=0\n",
    "        total_avr_sh_path=[]\n",
    "        GM=[]\n",
    "        PN=[]\n",
    "        MSC=0\n",
    "        total_c_sh_path=[]\n",
    "        total_bc_sh_path=[]\n",
    "        ndcg=[]\n",
    "        MAP=[]\n",
    "        for test_id in test_statistics.keys():\n",
    "\n",
    "            num_true_cntrs=len(test_statistics[test_id][\"true\"])\n",
    "            end=start+len(cntrs_ids)\n",
    "            c_scores=baseline_results[start:end]\n",
    "            start=end\n",
    "            c_ids=cntrs_ids.copy()\n",
    "            sorted_c_scores,sorted_c_ids=(list(t) for t in zip(*sorted(zip(c_scores, c_ids),reverse=True)) )\n",
    "                        \n",
    "            true_cntrs=test_statistics[test_id][\"true\"]\n",
    "            hat_cntrs=sorted_c_ids[:team_size]\n",
    "        \n",
    "            match=len(set(true_cntrs).intersection(set(hat_cntrs)))/len(true_cntrs)\n",
    "            GM.append(match)\n",
    "            if match>0:\n",
    "                MSC+=1\n",
    "                \n",
    "            match=len(set(true_cntrs).intersection(set(hat_cntrs)))/len(hat_cntrs)\n",
    "            PN.append(match)\n",
    "            \n",
    "            true_ranks=test_statistics[test_id][\"adds\"]\n",
    "            true_ranks=np.array(true_ranks)\n",
    "            true_ranks=(true_ranks/true_ranks.sum())*15+4\n",
    "            \n",
    "            true_scores=np.zeros(len(sorted_c_ids))\n",
    "            #print(true_ranks)\n",
    "            #print(true_cntrs)\n",
    "            hat_score=[]\n",
    "            for ii in range(len(true_cntrs)):\n",
    "                eid=true_cntrs[ii]\n",
    "                scoree=true_ranks[ii]\n",
    "                eindex=sorted_c_ids.index(eid)\n",
    "                true_scores[eindex]=scoree\n",
    "                hat_score.append(sorted_c_scores[eindex])\n",
    "            #print(true_scores)\n",
    "            \n",
    "            hat_score=np.array(hat_score)\n",
    "           # print(hat_score)\n",
    "           # print(true_ranks)\n",
    "            if min(hat_score)<0:\n",
    "                hat_score=hat_score-min(hat_score) \n",
    "                \n",
    "#             print(ndcg_score([hat_score], [true_ranks], k=team_size))\n",
    "            if min(sorted_c_scores)<0:\n",
    "                sorted_c_scores=sorted_c_scores-min(sorted_c_scores) \n",
    "                        \n",
    "            \n",
    "            #ndcg.append(ndcg_score([true_scores], [sorted_c_scores],  k=team_size))\n",
    "            ndcg.append(ndcg_score([true_ranks], [hat_score], k=team_size))\n",
    "            \n",
    "            sorted_true_scores,sorted_true_ids=(list(t) for t in zip(*sorted(zip(true_ranks, true_cntrs),reverse=True)) )\n",
    "            mapatk=ml_metrics.mapk([sorted_true_ids], [hat_cntrs],k=team_size)\n",
    "            MAP.append(mapatk)\n",
    "            \n",
    "            #compute average shortest path\n",
    "            avg_sh_c,num_path_c=0,0\n",
    "            hat_cntrs=list(set(hat_cntrs))\n",
    "            hat_cntrs.sort()\n",
    "            for i in range(len(hat_cntrs)-1):\n",
    "                for j in range(i+1,len(hat_cntrs)):\n",
    "                    ci=hat_cntrs[i]\n",
    "                    cj=hat_cntrs[j]\n",
    "                    sh_p=sh_path[str(ci)+\",\"+str(cj)]\n",
    "                    avg_sh_c+=sh_p\n",
    "                    num_path_c+=1\n",
    "                        \n",
    "            if num_path_c==0:\n",
    "                total_bc_sh_path.append(0)\n",
    "            else:\n",
    "                total_bc_sh_path.append(avg_sh_c/num_path_c)\n",
    "                    \n",
    "            t_c_shp=0\n",
    "            for j in range(len(hat_cntrs)):\n",
    "                    ri=test_id\n",
    "                    cj=hat_cntrs[j]\n",
    "                    if int(cj)>int(ri):\n",
    "                        ind=str(ri)+\",\"+str(cj)\n",
    "                    else:\n",
    "                        ind=str(cj)+\",\"+str(ri)\n",
    "                    \n",
    "                    t_c_shp+=sh_path[ind]\n",
    "                    \n",
    "            \n",
    "            total_c_sh_path.append(t_c_shp/len(hat_cntrs))\n",
    "            \n",
    "        total_c_sh_path=np.array(total_c_sh_path)\n",
    "        \n",
    "        GM=np.array(GM)\n",
    "        print(\"mean GM=\"+str(np.mean(GM)))\n",
    "        \n",
    "        PN=np.array(PN)\n",
    "        print(\"mean PN=\"+str(np.mean(PN)))\n",
    "        \n",
    "        MSC=MSC/len(test_statistics.keys())\n",
    "        \n",
    "        print(\"MSC=\",MSC)\n",
    "        \n",
    "        av_ndcg=np.mean(ndcg)\n",
    "        \n",
    "        print(\"ndcg=\",av_ndcg)\n",
    "        av_map=np.mean(np.array(MAP))\n",
    "        print(\"map=\",av_map)\n",
    "        \n",
    "        print(\"\\n\\nmean shortest path from test repo to selected es=\"+str(np.mean(total_c_sh_path)))\n",
    "        print(\"var shortest path from test repo to selected es=\"+str(np.var(total_c_sh_path)))\n",
    "        print(\"Standard Deviation shortest path from test repo to selected es=\"+str(np.std(total_c_sh_path)))\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\nmean shortest path between es=\"+str(np.mean(total_bc_sh_path)))\n",
    "        print(\"var shortest path between es=\"+str(np.var(total_bc_sh_path)))\n",
    "        print(\"Standard Deviation shortest path between es=\"+str(np.std(total_bc_sh_path)))\n",
    "        \n",
    "        print(\"dicovering teams done!\")\n",
    "        return round(np.mean(GM),3),round(np.mean(PN),3),round(\n",
    "            MSC,3),round(np.mean(total_c_sh_path),3), round(np.mean(total_bc_sh_path),3), round(av_ndcg,3),round(av_map,3)\n",
    "    \n",
    "    def discover_teams_TF_cikm2021(self,path,alg): \n",
    "        \"\"\"compute cikm metrics GM,PN, MSC, ASPL_c\"\"\"\n",
    "        #load results\n",
    "        test_statistics={}\n",
    "        fin=open(path+\"/Data_LDA_Topics/test.txt\",\"r\")\n",
    "        line=fin.readline().strip()\n",
    "        test_ids=[]\n",
    "        while line:\n",
    "            data=line.split(\",\")\n",
    "            repo_id=data[0].strip()\n",
    "            test_ids.append(repo_id)\n",
    "            if repo_id not in test_statistics:\n",
    "                test_statistics[repo_id]={\"true\":[int(data[5])]}\n",
    "            else:\n",
    "                test_statistics[repo_id][\"true\"].append(int(data[5]))\n",
    "            line=fin.readline().strip()\n",
    "        fin.close()\n",
    "        \n",
    "        test_res={}\n",
    "        fin=open(path+\"/Data_LDA_Topics/DBLPformat/all\"+alg+\"results.txt\",\"r\")\n",
    "        line=fin.readline()\n",
    "        while line:\n",
    "            r=line.strip().split(\" \")\n",
    "            team=[]\n",
    "            for c in r[1:]:\n",
    "                if int(c) not in team:\n",
    "                    team.append(int(c))\n",
    "            test_id=test_ids[int(r[0])]\n",
    "            test_res[test_id]=team\n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        \n",
    "        #shortest path\n",
    "        G_avr_sh_p=0\n",
    "        num_p=0\n",
    "        sh_path={}\n",
    "        fin=open(path+\"/Data_LDA_Topics/G_shortest_path.txt\",\"r\")\n",
    "        line=fin.readline()\n",
    "        while line:\n",
    "            data=line.split(\" \")\n",
    "            i=data[0]+\",\"+data[1]\n",
    "            sh=int(data[3])\n",
    "            #print(i,sh)\n",
    "            sh_path[i]=sh\n",
    "            G_avr_sh_p+=sh\n",
    "            num_p+=1\n",
    "            #sys.exit(0)\n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        print(\"G avr sh path=\"+str(G_avr_sh_p/num_p))\n",
    "        \n",
    "        #load embeddings        \n",
    "        # Load model from file\n",
    "#         with open(path+\"/st_co/\"+model, 'rb') as file:\n",
    "#             ob = pickle.load(file)\n",
    "        #print(\"model was loaded!!\")  \n",
    "        \n",
    "        fin=open(path +\"/Data_LDA_Topics/G_properties.txt\",\"r\",encoding=\"utf-8\")\n",
    "        self.N=int(fin.readline().split(\"=\")[1])\n",
    "        self.NumRepos=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        self.NumExperts=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        fin.close() \n",
    "        \n",
    "\n",
    "        \n",
    "        total_avr_sh_path=[]\n",
    "        \n",
    "        GM=[]\n",
    "        PN=[]\n",
    "        MSC=0\n",
    "        total_c_sh_path=[]\n",
    "        total_bc_sh_path=[]\n",
    "        av_team_size=0\n",
    "        for test_id in test_res.keys():\n",
    "            \n",
    "            avg_sh=0\n",
    "            num_path=0\n",
    "                        \n",
    "            true_cntrs=test_statistics[test_id][\"true\"]  #true team  \n",
    "            \n",
    "            hat_cntrs=test_res[test_id]  #discoverd team with size team_size\n",
    "            \n",
    "            av_team_size+=len(hat_cntrs)\n",
    "            \n",
    "            #compute average shortest path\n",
    "            avg_sh_c,num_path_c=0,0\n",
    "            \n",
    "            hat_cntrs=list(set(hat_cntrs))\n",
    "            match=len(set(hat_cntrs).intersection(set(true_cntrs)))/len(true_cntrs)\n",
    "            GM.append(match)\n",
    "            \n",
    "            if match >0:\n",
    "                MSC+=1\n",
    "            \n",
    "            match=len(set(hat_cntrs).intersection(set(true_cntrs)))/len(hat_cntrs)\n",
    "            PN.append(match)\n",
    "            \n",
    "            hat_cntrs.sort()\n",
    "            for i in range(len(hat_cntrs)-1):\n",
    "                for j in range(i+1,len(hat_cntrs)):\n",
    "                    ci=hat_cntrs[i]\n",
    "                    cj=hat_cntrs[j]\n",
    "                    sh_p=sh_path[str(ci)+\",\"+str(cj)]\n",
    "                    avg_sh_c+=sh_p\n",
    "                    num_path_c+=1\n",
    "            \n",
    "            if num_path_c==0:\n",
    "                total_bc_sh_path.append(0)\n",
    "            else:\n",
    "                total_bc_sh_path.append(avg_sh_c/num_path_c)\n",
    "                    \n",
    "            t_c_shp=0\n",
    "            for j in range(len(hat_cntrs)):\n",
    "                    ri=test_id\n",
    "                    cj=hat_cntrs[j]\n",
    "                    if int(cj)>int(ri):\n",
    "                        ind=str(ri)+\",\"+str(cj)\n",
    "                    else:\n",
    "                        ind=str(cj)+\",\"+str(ri)\n",
    "                    \n",
    "                    t_c_shp+=sh_path[ind]\n",
    "            \n",
    "            total_c_sh_path.append(t_c_shp/len(hat_cntrs))\n",
    "        \n",
    "        total_c_sh_path=np.array(total_c_sh_path)\n",
    "        \n",
    "        GM=np.array(GM)\n",
    "        print(\"mean GM=\"+str(np.mean(GM)))\n",
    "        \n",
    "        PN=np.array(PN)\n",
    "        print(\"mean PN=\"+str(np.mean(PN)))\n",
    "        \n",
    "        MSC=MSC/len(test_res.keys())\n",
    "        \n",
    "        print(\"MSC=\",MSC)\n",
    "        print(\"avr team size=\", av_team_size/len(test_res.keys() ))\n",
    "        \n",
    "        print(\"\\n\\nmean shortest path from test repo to selected es=\"+str(np.mean(total_c_sh_path)))\n",
    "        print(\"var shortest path from test repo to selected es=\"+str(np.var(total_c_sh_path)))\n",
    "        print(\"Standard Deviation shortest path from test repo to selected es=\"+str(np.std(total_c_sh_path)))\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\nmean shortest path between es=\"+str(np.mean(total_bc_sh_path)))\n",
    "        print(\"var shortest path between es=\"+str(np.var(total_bc_sh_path)))\n",
    "        print(\"Standard Deviation shortest path between es=\"+str(np.std(total_bc_sh_path)))\n",
    "        \n",
    "        print(\"dicovering teams done!\")\n",
    "        return round(np.mean(GM),5),round(np.mean(PN),5),round(\n",
    "            MSC,5),round(np.mean(total_c_sh_path),3), round(np.mean(total_bc_sh_path),3)\n",
    "    \n",
    "    def discover_teams_propose_vs_TF_cikm2021(self,path,contrs_ranking_results,model,alg): \n",
    "        \"\"\"compute cikm metrics GM,PN, MSC, ASPL_c\"\"\"\n",
    "        #load test results\n",
    "        test_conts_ranks={}\n",
    "        fin=open(path+\"/st_co/\"+contrs_ranking_results,\"r\")\n",
    "        line=fin.readline()\n",
    "        test_ids=[]\n",
    "        while line:\n",
    "            data=line.split(\",\")\n",
    "            repo_id=data[0]\n",
    "            true_contrs=[int(ids) for ids in data[1].split(\" \")]\n",
    "            neg_contrs=[int(ids) for ids in data[2].split(\" \")]\n",
    "            ids_ranks=data[3].split(\" \")\n",
    "            ids=[int(id_) for id_ in ids_ranks[0::2]]\n",
    "            ranks=[float(r) for r in ids_ranks[1::2]]\n",
    "            \n",
    "            test_conts_ranks[repo_id]={\"true\":true_contrs,\"neg\":neg_contrs,\"ids\":ids,\"sim\":ranks}\n",
    "            test_ids.append(repo_id)\n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        \n",
    "        test_res={}\n",
    "        fin=open(path+\"/DBLPformat/all\"+alg+\"results.txt\",\"r\")\n",
    "        line=fin.readline()\n",
    "        while line:\n",
    "            r=line.strip().split(\" \")\n",
    "            team=[]\n",
    "            for c in r[1:]:\n",
    "                if int(c) not in team:\n",
    "                    team.append(int(c))\n",
    "            test_id=test_ids[int(r[0])]\n",
    "            test_res[test_id]=team\n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        \n",
    "        #shortest path\n",
    "        G_avr_sh_p=0\n",
    "        num_p=0\n",
    "        sh_path={}\n",
    "        fin=open(path+\"G_shortest_path.txt\",\"r\")\n",
    "        line=fin.readline()\n",
    "        while line:\n",
    "            data=line.split(\" \")\n",
    "            i=data[0]+\",\"+data[1]\n",
    "            sh=int(data[3])\n",
    "            #print(i,sh)\n",
    "            sh_path[i]=sh\n",
    "            G_avr_sh_p+=sh\n",
    "            num_p+=1\n",
    "            #sys.exit(0)\n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        print(\"G avr sh path=\"+str(G_avr_sh_p/num_p))\n",
    "        \n",
    "        #load embeddings        \n",
    "        # Load model from file\n",
    "#         with open(path+\"/st_co/\"+model, 'rb') as file:\n",
    "#             ob = pickle.load(file)\n",
    "        #print(\"model was loaded!!\")  \n",
    "        \n",
    "        fin=open(path +\"G_properties.txt\",\"r\",encoding=\"utf-8\")\n",
    "        self.N=int(fin.readline().split(\"=\")[1])\n",
    "        self.NumRepos=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        self.NumExperts=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        fin.close() \n",
    "        \n",
    "       \n",
    "        total_avr_sh_path=[]\n",
    "        \n",
    "        GM=[]\n",
    "        PN=[]\n",
    "        MSC=0\n",
    "        total_c_sh_path=[]\n",
    "        total_bc_sh_path=[]\n",
    "        \n",
    "        for test_id in test_res.keys():\n",
    "            avg_sh=0\n",
    "            num_path=0\n",
    "            team_size=len(test_res[test_id])\n",
    "            \n",
    "            test=test_conts_ranks[test_id]\n",
    "\n",
    "            res=str(test_id)\n",
    "            \n",
    "            true_cntrs=test[\"true\"]  #true team            \n",
    "            hat_cntrs=test[\"ids\"][:team_size]  #discoverd team with size team_size\n",
    "            \n",
    "            \n",
    "            \n",
    "            #compute average shortest path\n",
    "            avg_sh_c,num_path_c=0,0\n",
    "            \n",
    "            hat_cntrs=list(set(hat_cntrs))\n",
    "            match=len(set(hat_cntrs).intersection(set(true_cntrs)))/len(true_cntrs)\n",
    "            GM.append(match)\n",
    "            \n",
    "            if match >0:\n",
    "                MSC+=1\n",
    "            \n",
    "            match=len(set(hat_cntrs).intersection(set(true_cntrs)))/len(hat_cntrs)\n",
    "            PN.append(match)\n",
    "            \n",
    "            hat_cntrs.sort()\n",
    "            for i in range(len(hat_cntrs)-1):\n",
    "                for j in range(i+1,len(hat_cntrs)):\n",
    "                    ci=hat_cntrs[i]\n",
    "                    cj=hat_cntrs[j]\n",
    "                    sh_p=sh_path[str(ci)+\",\"+str(cj)]\n",
    "                    avg_sh_c+=sh_p\n",
    "                    num_path_c+=1\n",
    "            \n",
    "            if num_path_c==0:\n",
    "                total_bc_sh_path.append(0)\n",
    "            else:\n",
    "                total_bc_sh_path.append(avg_sh_c/num_path_c)\n",
    "                    \n",
    "            t_c_shp=0\n",
    "            for j in range(len(hat_cntrs)):\n",
    "                    ri=test_id\n",
    "                    cj=hat_cntrs[j]\n",
    "                    if int(cj)>int(ri):\n",
    "                        ind=str(ri)+\",\"+str(cj)\n",
    "                    else:\n",
    "                        ind=str(cj)+\",\"+str(ri)\n",
    "                    \n",
    "                    t_c_shp+=sh_path[ind]\n",
    "            \n",
    "            total_c_sh_path.append(t_c_shp/len(hat_cntrs))\n",
    "            \n",
    "        \n",
    "        \n",
    "       \n",
    "        total_c_sh_path=np.array(total_c_sh_path)\n",
    "        \n",
    "        GM=np.array(GM)\n",
    "        print(\"mean GM=\"+str(np.mean(GM)))\n",
    "        \n",
    "        PN=np.array(PN)\n",
    "        print(\"mean PN=\"+str(np.mean(PN)))\n",
    "        \n",
    "        MSC=MSC/len(test_res.keys())\n",
    "        \n",
    "        print(\"MSC=\",MSC)\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\nmean shortest path from test repo to selected es=\"+str(np.mean(total_c_sh_path)))\n",
    "        print(\"var shortest path from test repo to selected es=\"+str(np.var(total_c_sh_path)))\n",
    "        print(\"Standard Deviation shortest path from test repo to selected es=\"+str(np.std(total_c_sh_path)))\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\nmean shortest path between es=\"+str(np.mean(total_bc_sh_path)))\n",
    "        print(\"var shortest path between es=\"+str(np.var(total_bc_sh_path)))\n",
    "        print(\"Standard Deviation shortest path between es=\"+str(np.std(total_bc_sh_path)))\n",
    "        \n",
    "        print(\"dicovering teams done!\")\n",
    "        return round(np.mean(GM),3),round(np.mean(PN),3),round(\n",
    "            MSC,3),round(np.mean(total_c_sh_path),3), round(np.mean(total_bc_sh_path),3)\n",
    "    \n",
    "    def discover_teams_cikm2021(self,path,contrs_ranking_results,model,team_size): \n",
    "        \"\"\"compute cikm metrics GM,PN, MSC, ASPL_c,ASPL_bc ndcg map\"\"\"\n",
    "        #load baseline results\n",
    "        test_statistics={}\n",
    "        fin=open(path+\"test.txt\",\"r\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            data=line.split(\",\")\n",
    "            repo_id=data[0].strip()\n",
    "            numadds=int(data[6].split(\" \")[0])\n",
    "            if repo_id not in test_statistics:\n",
    "                test_statistics[repo_id]={\"true\":[int(data[5])],\"adds\":[numadds],\"neg\":[]}\n",
    "            else:\n",
    "                test_statistics[repo_id][\"true\"].append(int(data[5]))\n",
    "                test_statistics[repo_id][\"adds\"].append(numadds)\n",
    "            line=fin.readline().strip()\n",
    "        fin.close()\n",
    "        \n",
    "        #load test results\n",
    "        test_conts_ranks={}\n",
    "        fin=open(path+\"/st_co/\"+contrs_ranking_results,\"r\")\n",
    "        line=fin.readline()\n",
    "        while line:\n",
    "            data=line.split(\",\")\n",
    "            repo_id=data[0]\n",
    "            true_contrs=[int(ids) for ids in data[1].split(\" \")]\n",
    "            neg_contrs=[int(ids) for ids in data[2].split(\" \")]\n",
    "            ids_ranks=data[3].split(\" \")\n",
    "            ids=[int(id_) for id_ in ids_ranks[0::2]]\n",
    "            ranks=[float(r) for r in ids_ranks[1::2]]\n",
    "            \n",
    "            test_conts_ranks[repo_id]={\"true\":true_contrs,\"neg\":neg_contrs,\"ids\":ids,\"sim\":ranks}\n",
    "            \n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #shortest path\n",
    "        G_avr_sh_p=0\n",
    "        num_p=0\n",
    "        sh_path={}\n",
    "        fin=open(path+\"G_shortest_path.txt\",\"r\")\n",
    "        line=fin.readline()\n",
    "        while line:\n",
    "            data=line.split(\" \")\n",
    "            i=data[0]+\",\"+data[1]\n",
    "            sh=int(data[3])\n",
    "            #print(i,sh)\n",
    "            sh_path[i]=sh\n",
    "            G_avr_sh_p+=sh\n",
    "            num_p+=1\n",
    "            #sys.exit(0)\n",
    "            line=fin.readline()\n",
    "        fin.close()\n",
    "        print(\"G avr sh path=\"+str(G_avr_sh_p/num_p))\n",
    "        \n",
    "        #load embeddings        \n",
    "        # Load model from file\n",
    "#         with open(path+\"/st_co/\"+model, 'rb') as file:\n",
    "#             ob = pickle.load(file)\n",
    "        #print(\"model was loaded!!\")  \n",
    "        \n",
    "        fin=open(path +\"G_properties.txt\",\"r\",encoding=\"utf-8\")\n",
    "        self.N=int(fin.readline().split(\"=\")[1])\n",
    "        self.NumRepos=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        self.NumExperts=int(fin.readline().split(\"=\")[1].split(\" \")[0])\n",
    "        fin.close() \n",
    "        \n",
    "        total_avr_sh_path=[]\n",
    "        fouts=open(path+\"st_co/\"+contrs_ranking_results[:-4]+\"_teams_teamsize\"+str(team_size)+\".txt\",\"w\")\n",
    "        GM=[]\n",
    "        PN=[]\n",
    "        MSC=0\n",
    "        total_c_sh_path=[]\n",
    "        total_bc_sh_path=[]\n",
    "        ndcg=[]\n",
    "        MAP=[]\n",
    "        for test_id in test_conts_ranks.keys():\n",
    "            avg_sh=0\n",
    "            num_path=0\n",
    "            test=test_conts_ranks[test_id]\n",
    "\n",
    "            res=str(test_id)\n",
    "            \n",
    "            true_cntrs=test[\"true\"]  #true team            \n",
    "            hat_cntrs=test[\"ids\"][:team_size]  #discoverd team with size team_size\n",
    "            \n",
    "            res+=\",\"+\" \".join([str(c) for c in hat_cntrs]).strip()\n",
    "            \n",
    "            fouts.write(res+\"\\n\")\n",
    "            fouts.flush()\n",
    "            \n",
    "            #compute average shortest path\n",
    "            avg_sh_c,num_path_c=0,0\n",
    "            \n",
    "            hat_cntrs=list(set(hat_cntrs))\n",
    "            match=len(set(hat_cntrs).intersection(set(true_cntrs)))/len(true_cntrs)\n",
    "            GM.append(match)\n",
    "            \n",
    "            if match >0:\n",
    "                MSC+=1\n",
    "            \n",
    "            match=len(set(hat_cntrs).intersection(set(true_cntrs)))/len(hat_cntrs)\n",
    "            PN.append(match)\n",
    "            \n",
    "            true_ranks=test_statistics[test_id][\"adds\"]\n",
    "            true_ranks=np.array(true_ranks)\n",
    "            true_ranks=(true_ranks/true_ranks.sum())*15+4\n",
    "            \n",
    "            sorted_c_scores=np.array(test[\"sim\"])\n",
    "            true_scores=np.zeros(len(test[\"ids\"]))\n",
    "            hat_score=[]\n",
    "            for ii in range(len(true_cntrs)):\n",
    "                eid=true_cntrs[ii]\n",
    "                scoree=true_ranks[ii]\n",
    "                eindex=test[\"ids\"].index(eid)\n",
    "                true_scores[eindex]=scoree\n",
    "                hat_score.append(sorted_c_scores[eindex])\n",
    "            #print(true_scores)\n",
    "            \n",
    "            hat_score=np.array(hat_score)\n",
    "            if min(hat_score)<0:\n",
    "                hat_score=hat_score-min(hat_score) \n",
    "                \n",
    "#             print(ndcg_score([hat_score], [true_ranks], k=team_size))\n",
    "            if min(sorted_c_scores)<0:\n",
    "                sorted_c_scores=sorted_c_scores-min(sorted_c_scores) \n",
    "                        \n",
    "            \n",
    "            #ndcg.append(ndcg_score([true_scores], [sorted_c_scores],  k=team_size))\n",
    "            ndcg.append(ndcg_score([true_ranks], [hat_score], k=team_size))\n",
    "            \n",
    "            sorted_true_scores,sorted_true_ids=(list(t) for t in zip(*sorted(zip(true_ranks, true_cntrs),reverse=True)) )\n",
    "            mapatk=ml_metrics.mapk([sorted_true_ids], [hat_cntrs],k=team_size)\n",
    "            MAP.append(mapatk)\n",
    "            \n",
    "            \n",
    "            \n",
    "            hat_cntrs.sort()\n",
    "            for i in range(len(hat_cntrs)-1):\n",
    "                for j in range(i+1,len(hat_cntrs)):\n",
    "                    ci=hat_cntrs[i]\n",
    "                    cj=hat_cntrs[j]\n",
    "                    sh_p=sh_path[str(ci)+\",\"+str(cj)]\n",
    "                    avg_sh_c+=sh_p\n",
    "                    num_path_c+=1\n",
    "            \n",
    "            if num_path_c==0:\n",
    "                total_bc_sh_path.append(0)\n",
    "            else:\n",
    "                total_bc_sh_path.append(avg_sh_c/num_path_c)\n",
    "                    \n",
    "            t_c_shp=0\n",
    "            for j in range(len(hat_cntrs)):\n",
    "                    ri=test_id\n",
    "                    cj=hat_cntrs[j]\n",
    "                    if int(cj)>int(ri):\n",
    "                        ind=str(ri)+\",\"+str(cj)\n",
    "                    else:\n",
    "                        ind=str(cj)+\",\"+str(ri)\n",
    "                    \n",
    "                    t_c_shp+=sh_path[ind]\n",
    "            \n",
    "            total_c_sh_path.append(t_c_shp/len(hat_cntrs))\n",
    "            \n",
    "        \n",
    "        fouts.close()\n",
    "       \n",
    "        total_c_sh_path=np.array(total_c_sh_path)\n",
    "        \n",
    "        GM=np.array(GM)\n",
    "        print(\"mean GM=\"+str(np.mean(GM)))\n",
    "        \n",
    "        PN=np.array(PN)\n",
    "        print(\"mean PN=\"+str(np.mean(PN)))\n",
    "        \n",
    "        MSC=MSC/len(test_conts_ranks.keys())\n",
    "        \n",
    "        print(\"MSC=\",MSC)\n",
    "        \n",
    "        av_ndcg=np.mean(ndcg)\n",
    "        \n",
    "        print(\"ndcg=\",av_ndcg)\n",
    "        av_map=np.mean(np.array(MAP))\n",
    "        print(\"map=\",av_map)\n",
    "        \n",
    "        print(\"\\n\\nmean shortest path from test repo to selected es=\"+str(np.mean(total_c_sh_path)))\n",
    "        print(\"var shortest path from test repo to selected es=\"+str(np.var(total_c_sh_path)))\n",
    "        print(\"Standard Deviation shortest path from test repo to selected es=\"+str(np.std(total_c_sh_path)))\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\nmean shortest path between es=\"+str(np.mean(total_bc_sh_path)))\n",
    "        print(\"var shortest path between es=\"+str(np.var(total_bc_sh_path)))\n",
    "        print(\"Standard Deviation shortest path between es=\"+str(np.std(total_bc_sh_path)))\n",
    "        \n",
    "        print(\"dicovering teams done!\")\n",
    "        return round(np.mean(GM),3),round(np.mean(PN),3),round(\n",
    "            MSC,3),round(np.mean(total_c_sh_path),3), round(np.mean(total_bc_sh_path),3),round(av_ndcg,3),round(av_map,3)\n",
    "\n",
    "dataset=[\"CNCF\",\"JAVA\",\"ML\",\"NE\"] \n",
    "data=dataset[0]\n",
    "\n",
    "\n",
    "trian=True\n",
    "print(\"Start\")\n",
    "if trian==True:\n",
    "    ob=CoExperts(data)\n",
    "    ob.train(\"a\",0.0005,5e-6,500,15) \n",
    "else:  \n",
    "    CoExperts.test_all(data,\"/Data_LDA_Topics/\",\"pickle_model.pkl_i\",str(1))    \n",
    "print(\"Done!\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
